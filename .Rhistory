data_raw <- data_func(data,data0,pre_time,lookback_time)
data_feature <- data_feature_func_Auto(data_raw,lookback_time, latent_dim)
#'
data_feature_func_Auto <- function(data_raw,lookback_time, latent_dim){
library(tidyverse)
library(keras)
library(tensorflow)
library(kernlab)
#Define an empty list
data_feature_list <- vector('list',length(lookback_time))
for(j in 1:length(lookback_time)){
#Iterate over the data
data <- data_raw[[j]]
#z-score standardised
data <- data %>%
mutate_at(vars(colnames(data)[3:11]),scale)
# Determining record length
record_length <- lookback_time[j]
# Determine the number of features as 8
num_features <- 8
##Selection of features for Autoencoder
selected_lookback <- data %>%
select(hadm_id, sbp_avg, dbp_avg, ph_avg, so2_avg, temper_avg, heart_rate_avg, res_rate_avg, wc_avg)
## Convert into an array
## Group by hadm_id
grouped_data <- selected_lookback %>%
group_by(hadm_id) %>%
mutate(row_number = row_number()) %>%
ungroup()
# Get the unique hadm_id
unique_hadm_ids <- unique(grouped_data$hadm_id)
# Initialise an empty array to store the results
result_array <- array(NA, dim = c(length(unique_hadm_ids), record_length, num_features))
#Fill the array
for (i in 1:length(unique_hadm_ids)) {
hadm_id <- unique_hadm_ids[i]
hadm_data <- grouped_data[grouped_data$hadm_id == hadm_id, ]
subset_data <- hadm_data[, c("sbp_avg", "dbp_avg", "ph_avg", "so2_avg", "temper_avg", "heart_rate_avg", "res_rate_avg", "wc_avg")]
# Convert subset_data to a matrix
subset_matrix <- as.matrix(subset_data)
# Fill subset_matrix to the corresponding position in result_array
result_array[i,,] <- subset_matrix
}
# Build autoencoder models
input_shape <- dim(result_array)[2:3]
#latent_dim <- 5  # Dimensions of potential space
encoder <- keras_model_sequential() %>%
layer_flatten(input_shape = input_shape) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = latent_dim)
decoder <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = c(latent_dim)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = prod(input_shape), activation = "sigmoid") %>%
layer_reshape(target_shape = input_shape)
input_tensor <- layer_input(shape = input_shape)
encoded_tensor <- encoder(input_tensor)
decoded_tensor <- decoder(encoded_tensor)
autoencoder <- keras_model(inputs = input_tensor, outputs = decoded_tensor)
# compilation model
autoencoder %>% compile(optimizer = "adam", loss = "mse")
# Run model training
history <- autoencoder %>% fit(result_array, result_array, epochs = 50, batch_size = 32, validation_split = 0.2)
# Encoding the data using the trained model
encoded_data <- as.data.frame(predict(encoder, result_array))
#add hadm_id
encoded_data <- cbind(hadm_id = unique(data$hadm_id),encoded_data)
grouped_measurements <- data %>%
group_by(hadm_id) %>%
# Calculate maximum, minimum and variance for each indicator
summarize(
sepsis_diagnose = first(sepsis_diagnose), # Use the first() function to get a constant value
anchor_age = first(anchor_age),
sbp_max = max(sbp_avg),
sbp_min = min(sbp_avg),
sbp_var = var(sbp_avg),
dbp_max = max(dbp_avg),
dbp_min = min(dbp_avg),
dbp_var = var(dbp_avg),
ph_max = max(ph_avg),
ph_min = min(ph_avg),
ph_var = var(ph_avg),
so2_max = max(so2_avg),
so2_min = min(so2_avg),
so2_var = var(so2_avg),
temper_max = max(temper_avg),
temper_min = min(temper_avg),
temper_var = var(temper_avg),
heart_rate_max = max(heart_rate_avg),
heart_rate_min = min(heart_rate_avg),
heart_rate_var = var(heart_rate_avg),
res_rate_max = max(res_rate_avg),
res_rate_min = min(res_rate_avg),
res_rate_var = var(res_rate_avg),
wc_max = max(wc_avg),
wc_min = min(wc_avg),
wc_var = var(wc_avg)
)
# Merge two dataframes by hadm_id using the left_join() function
data_1 <- left_join(grouped_measurements, encoded_data, by = "hadm_id")
data_feature_list[[j]] <- data_1
}
return(data_feature_list)
}
data_feature <- data_feature_func_Auto(data_raw,lookback_time, latent_dim)
#'
data_feature_func_Auto <- function(data_raw,lookback_time, latent_dim){
library(tidyverse)
library(keras3)
library(tensorflow)
library(kernlab)
#Define an empty list
data_feature_list <- vector('list',length(lookback_time))
for(j in 1:length(lookback_time)){
#Iterate over the data
data <- data_raw[[j]]
#z-score standardised
data <- data %>%
mutate_at(vars(colnames(data)[3:11]),scale)
# Determining record length
record_length <- lookback_time[j]
# Determine the number of features as 8
num_features <- 8
##Selection of features for Autoencoder
selected_lookback <- data %>%
select(hadm_id, sbp_avg, dbp_avg, ph_avg, so2_avg, temper_avg, heart_rate_avg, res_rate_avg, wc_avg)
## Convert into an array
## Group by hadm_id
grouped_data <- selected_lookback %>%
group_by(hadm_id) %>%
mutate(row_number = row_number()) %>%
ungroup()
# Get the unique hadm_id
unique_hadm_ids <- unique(grouped_data$hadm_id)
# Initialise an empty array to store the results
result_array <- array(NA, dim = c(length(unique_hadm_ids), record_length, num_features))
#Fill the array
for (i in 1:length(unique_hadm_ids)) {
hadm_id <- unique_hadm_ids[i]
hadm_data <- grouped_data[grouped_data$hadm_id == hadm_id, ]
subset_data <- hadm_data[, c("sbp_avg", "dbp_avg", "ph_avg", "so2_avg", "temper_avg", "heart_rate_avg", "res_rate_avg", "wc_avg")]
# Convert subset_data to a matrix
subset_matrix <- as.matrix(subset_data)
# Fill subset_matrix to the corresponding position in result_array
result_array[i,,] <- subset_matrix
}
# Build autoencoder models
input_shape <- dim(result_array)[2:3]
#latent_dim <- 5  # Dimensions of potential space
encoder <- keras_model_sequential() %>%
layer_flatten(input_shape = input_shape) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = latent_dim)
decoder <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = c(latent_dim)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = prod(input_shape), activation = "sigmoid") %>%
layer_reshape(target_shape = input_shape)
input_tensor <- layer_input(shape = input_shape)
encoded_tensor <- encoder(input_tensor)
decoded_tensor <- decoder(encoded_tensor)
autoencoder <- keras_model(inputs = input_tensor, outputs = decoded_tensor)
# compilation model
autoencoder %>% compile(optimizer = "adam", loss = "mse")
# Run model training
history <- autoencoder %>% fit(result_array, result_array, epochs = 50, batch_size = 32, validation_split = 0.2)
# Encoding the data using the trained model
encoded_data <- as.data.frame(predict(encoder, result_array))
#add hadm_id
encoded_data <- cbind(hadm_id = unique(data$hadm_id),encoded_data)
grouped_measurements <- data %>%
group_by(hadm_id) %>%
# Calculate maximum, minimum and variance for each indicator
summarize(
sepsis_diagnose = first(sepsis_diagnose), # Use the first() function to get a constant value
anchor_age = first(anchor_age),
sbp_max = max(sbp_avg),
sbp_min = min(sbp_avg),
sbp_var = var(sbp_avg),
dbp_max = max(dbp_avg),
dbp_min = min(dbp_avg),
dbp_var = var(dbp_avg),
ph_max = max(ph_avg),
ph_min = min(ph_avg),
ph_var = var(ph_avg),
so2_max = max(so2_avg),
so2_min = min(so2_avg),
so2_var = var(so2_avg),
temper_max = max(temper_avg),
temper_min = min(temper_avg),
temper_var = var(temper_avg),
heart_rate_max = max(heart_rate_avg),
heart_rate_min = min(heart_rate_avg),
heart_rate_var = var(heart_rate_avg),
res_rate_max = max(res_rate_avg),
res_rate_min = min(res_rate_avg),
res_rate_var = var(res_rate_avg),
wc_max = max(wc_avg),
wc_min = min(wc_avg),
wc_var = var(wc_avg)
)
# Merge two dataframes by hadm_id using the left_join() function
data_1 <- left_join(grouped_measurements, encoded_data, by = "hadm_id")
data_feature_list[[j]] <- data_1
}
return(data_feature_list)
}
data_feature <- data_feature_func_Auto(data_raw,lookback_time, latent_dim)
#'
data_feature_func_Auto <- function(data_raw,lookback_time, latent_dim){
library(keras3)
library(tensorflow)
library(tidyverse)
library(dplyr)
library(caret)
library(caTools)
library(rpart)
library(caretEnsemble)
library(pROC)
library(keras)
library(reticulate)
library(kernlab)
#Define an empty list
data_feature_list <- vector('list',length(lookback_time))
for(j in 1:length(lookback_time)){
#Iterate over the data
data <- data_raw[[j]]
#z-score standardised
data <- data %>%
mutate_at(vars(colnames(data)[3:11]),scale)
# Determining record length
record_length <- lookback_time[j]
# Determine the number of features as 8
num_features <- 8
##Selection of features for Autoencoder
selected_lookback <- data %>%
select(hadm_id, sbp_avg, dbp_avg, ph_avg, so2_avg, temper_avg, heart_rate_avg, res_rate_avg, wc_avg)
## Convert into an array
## Group by hadm_id
grouped_data <- selected_lookback %>%
group_by(hadm_id) %>%
mutate(row_number = row_number()) %>%
ungroup()
# Get the unique hadm_id
unique_hadm_ids <- unique(grouped_data$hadm_id)
# Initialise an empty array to store the results
result_array <- array(NA, dim = c(length(unique_hadm_ids), record_length, num_features))
#Fill the array
for (i in 1:length(unique_hadm_ids)) {
hadm_id <- unique_hadm_ids[i]
hadm_data <- grouped_data[grouped_data$hadm_id == hadm_id, ]
subset_data <- hadm_data[, c("sbp_avg", "dbp_avg", "ph_avg", "so2_avg", "temper_avg", "heart_rate_avg", "res_rate_avg", "wc_avg")]
# Convert subset_data to a matrix
subset_matrix <- as.matrix(subset_data)
# Fill subset_matrix to the corresponding position in result_array
result_array[i,,] <- subset_matrix
}
# Build autoencoder models
input_shape <- dim(result_array)[2:3]
#latent_dim <- 5  # Dimensions of potential space
encoder <- keras_model_sequential() %>%
layer_flatten(input_shape = input_shape) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = 32, activation = "relu") %>%
layer_dense(units = latent_dim)
decoder <- keras_model_sequential() %>%
layer_dense(units = 32, activation = "relu", input_shape = c(latent_dim)) %>%
layer_dense(units = 64, activation = "relu") %>%
layer_dense(units = prod(input_shape), activation = "sigmoid") %>%
layer_reshape(target_shape = input_shape)
input_tensor <- layer_input(shape = input_shape)
encoded_tensor <- encoder(input_tensor)
decoded_tensor <- decoder(encoded_tensor)
autoencoder <- keras_model(inputs = input_tensor, outputs = decoded_tensor)
# compilation model
autoencoder %>% compile(optimizer = "adam", loss = "mse")
# Run model training
history <- autoencoder %>% fit(result_array, result_array, epochs = 50, batch_size = 32, validation_split = 0.2)
# Encoding the data using the trained model
encoded_data <- as.data.frame(predict(encoder, result_array))
#add hadm_id
encoded_data <- cbind(hadm_id = unique(data$hadm_id),encoded_data)
grouped_measurements <- data %>%
group_by(hadm_id) %>%
# Calculate maximum, minimum and variance for each indicator
summarize(
sepsis_diagnose = first(sepsis_diagnose), # Use the first() function to get a constant value
anchor_age = first(anchor_age),
sbp_max = max(sbp_avg),
sbp_min = min(sbp_avg),
sbp_var = var(sbp_avg),
dbp_max = max(dbp_avg),
dbp_min = min(dbp_avg),
dbp_var = var(dbp_avg),
ph_max = max(ph_avg),
ph_min = min(ph_avg),
ph_var = var(ph_avg),
so2_max = max(so2_avg),
so2_min = min(so2_avg),
so2_var = var(so2_avg),
temper_max = max(temper_avg),
temper_min = min(temper_avg),
temper_var = var(temper_avg),
heart_rate_max = max(heart_rate_avg),
heart_rate_min = min(heart_rate_avg),
heart_rate_var = var(heart_rate_avg),
res_rate_max = max(res_rate_avg),
res_rate_min = min(res_rate_avg),
res_rate_var = var(res_rate_avg),
wc_max = max(wc_avg),
wc_min = min(wc_avg),
wc_var = var(wc_avg)
)
# Merge two dataframes by hadm_id using the left_join() function
data_1 <- left_join(grouped_measurements, encoded_data, by = "hadm_id")
data_feature_list[[j]] <- data_1
}
return(data_feature_list)
}
data_feature <- data_feature_func_Auto(data_raw,lookback_time, latent_dim)
#' @export
#'
#' @example
#' data(eg_sep_data)
#' data(eg_nonsep_data)
#' pre_time <- 3
#' lookback_time <- c(2,4,6,8,10)
#' data_raw <- data_func(data,data0,pre_time,lookback_time)
#' data_feature <- data_feature_func_InSi(data_raw,lookback_time)
#'
data_feature_func_InSi <- function(data_raw,lookback_time){
library(keras3)
library(tensorflow)
library(tidyverse)
library(dplyr)
library(caret)
library(caTools)
library(rpart)
library(caretEnsemble)
library(pROC)
library(keras)
library(reticulate)
library(kernlab)
#Define an empty list
data_feature_list <- vector('list',length(lookback_time))
for(j in 1:length(lookback_time)){
#Iterate over the data
data <- data_raw[[j]]
#Calculate the mean of the 9 variables
mean_result <- data %>%
group_by(hadm_id) %>%
summarise(
sepsis_diagnose = first(sepsis_diagnose),
mean_anchor_age = mean(anchor_age),
mean_sbp = mean(sbp_avg),
mean_dbp = mean(dbp_avg),
mean_ph = mean(ph_avg),
mean_so2 = mean(so2_avg),
mean_temper = mean(temper_avg),
mean_heart_rate = mean(heart_rate_avg),
mean_res_rate = mean(res_rate_avg),
mean_wc = mean(wc_avg)
)
#Calculation of the difference between 8 variables except age
diff_result <- data %>%
group_by(hadm_id) %>%
summarise(
sbp_diff = first(sbp_avg) - last(sbp_avg),
dbp_diff = first(dbp_avg) - last(dbp_avg),
ph_diff = first(ph_avg) - last(ph_avg),
so2_diff = first(so2_avg) - last(so2_avg),
temper_diff = first(temper_avg) - last(temper_avg),
heart_rate_diff = first(heart_rate_avg) - last(heart_rate_avg),
res_rate_diff = first(res_rate_avg) - last(res_rate_avg),
wc_diff = first(wc_avg) - last(wc_avg)
)
#Consolidation of data
data_1 <- left_join(mean_result, diff_result, by = "hadm_id")
# Standardised processing
data_1 <- data_1 %>%
mutate_at(vars(colnames(data_1)[3:ncol(data_1)]), scale)
data_feature_list[[j]] <- data_1
}
return(data_feature_list)
}
#' @param lookback_time A vector representing the length of the cumulative time window.
#' @return A list containing the feature data frames extracted for each time window.
#' @export
#'
#' @example
#' data(eg_sep_data)
#' data(eg_nonsep_data)
#' pre_time <- 3
#' lookback_time <- c(2,4,6,8,10)
#' data_raw <- data_func(data,data0,pre_time,lookback_time)
data_feature <- data_feature_func_InSi(data_raw,lookback_time)
#source('E:/R code/sepsis/function_code.R')
source('E:/R code/sepsis/func_LSTM.R')
#read data
data <- read.csv('E:/脓毒症/sirs_5h_8h_连续完整版.csv')
install.packages("tensorflow")
install.packages("tensorflow")
data0 <- read.csv('E:/脓毒症/predicton_3h/nonsepsis_sample插值成功版.csv')
#parameter setting
pre_time <- 3
lookback_time <- c(2,4,6,8,10)
k <- 32   # Number of LSTM hidden layer neurons
z <- 5  # Number of neurons in the fully connected layer
#running result
data_raw <- data_func(data,data0,pre_time,lookback_time)
data_feature <- data_feature_func(data_raw,lookback_time)
library(reticulate)
py_install("keras")
py_install("tensorflow")
py_install("keras", pip = TRUE)
py_install("tensorflow", pip = TRUE)
library(keras)
library(keras3)
data_feature <- data_feature_func(data_raw,lookback_time)
#source('E:/R code/sepsis/function_code.R')
source('E:/R code/sepsis/func_LSTM.R')
data_feature <- data_feature_func(data_raw,lookback_time)
library(CTSM)
data(eg_sep_data)
data(eg_nonsep_data)
pre_time <- 3
lookback_time <- c(2,4,6,8,10)
latent_dim <- 5
data_raw <- data_func(data,data0,pre_time,lookback_time)
View(data0)
data_feature <- data_feature_func_Auto(data_raw,lookback_time,latent_dim)
View(data_feature)
View(data_feature[[1]])
devtools::document()
devtools::install()
library(CTSM)
data(eg_sep_data)
data(eg_nonsep_data)
pre_time <- 3
lookback_time <- c(2,4,6,8,10)
latent_dim <- 5
data_raw <- data_func(data,data0,pre_time,lookback_time)
data_feature <- data_feature_func_Auto(data_raw,lookback_time,latent_dim)
k <- 32
z <- 5
data_feature <- data_feature_func_LSTM(data_raw,lookback_time,k,z)
View(data_feature)
data_feature <- data_feature_func_Auto(data_raw,lookback_time,latent_dim)
View(data_feature)
data_feature <- data_feature_func_InSi(data_raw,lookback_time)
View(data_feature)
data_feature <- data_feature_func_KPCA(data_raw,lookback_time)
View(data_feature)
data_feature <- data_feature_func_PCA(data_raw,lookback_time)
data_feature <- data_feature_func_KPCA(data_raw,lookback_time)
data_stacking <- stacking_func(data_feature,lookback_time)
n_1 <- dim(data)
n_1
n_1 <- ncol(data)
n_hadm <- length(unique(data$hadm_id))
n_hadm
devtools::document()
devtools::install()
library(CTSM)
data(eg_sep_data)
data(eg_nonsep_data)
library(devtools)
library(knitr)
devtools::document()
devtools::document()
devtools::document()
data(eg_sep_data)
data(eg_nonsep_data)
devtools::document()
data(eg_sep_data)
data(eg_nonsep_data)
devtools::document()
devtools::document()
devtools::document()
devtools::document()
devtools::document()
n_hadm <- length(unique(data$hadm_id))
devtools::document()
devtools::document()
devtools::document()
devtools::install()
library(CTSM)
devtools::document()
devtools::install()
library(CTSM)
data(eg_sep_data)
data(eg_nonsep_data)
n_var <- ncol(data)
n_hadm <- length(unique(data$hadm_id))
pre_time <- 3
lookback_time <- c(2,4,6,8,10)
latent_dim <- 5
k <- 32
z <- 5
data_raw <- data_func(data,data0,pre_time,lookback_time)
devtools::document()
devtools::install()
library(CTSM)
data(eg_sep_data)
data(eg_nonsep_data)
n_var <- ncol(data)
n_hadm <- length(unique(data$hadm_id))
pre_time <- 3
lookback_time <- c(2,4,6,8,10)
data_raw <- data_func(data,data0,pre_time,lookback_time)
